{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Handwritten Digit Recognition\n",
    "* Design a neural network to recognize ten handwritten digits, 0-9. This is a multiclass classification task where one of n choices is selected. Automated handwritten digit recognition is widely used today - from recognizing zip codes (postal codes) on mail envelopes to recognizing amounts written on bank check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rectified linear unit (ReLU) type activation function is defined as follows,\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    a=max(0,z)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(z):  \n",
    "    \"\"\" Softmax converts a vector of values to a probability distribution.\n",
    "    Args:\n",
    "      z (ndarray (N,))  : input data, N features\n",
    "    Returns:\n",
    "      a (ndarray (N,))  : softmax of z\n",
    "    \"\"\"    \n",
    "    ### START CODE HERE ### \n",
    "    a=np.exp(z)/np.sum(np.exp(z))\n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load the data into variables X and y\n",
    "    * The data set contains 5000 training examples of handwritten digits.\n",
    "    * Each training example is a 20-pixel x 20-pixel grayscale image of a digit.\n",
    "    * Each pixel is represented by a floating-point number indicating the grayscale intensity at that location.\n",
    "    * The 20 by 20 grid of pixels is **unrolled** into a 1 by 400 row vector.\n",
    "    * Each training examples becomes a single row in the data matrix X, which gives us a 5000 x 400 matrix X. Thus the shape of X is: (5000, 400) and the shape of y is: (5000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model representation has two dense layers with ReLU activations followed by an output layer with a linear activation and a softmax activation. The parameters have dimensions that are sized for a neural network with  25 units in layer 1,  15 units in layer 2 and  10 output units in layer 3 each of which is for a digit.\n",
    "*  The numerical stability is improved if the softmax is grouped with the loss function rather than the output layer during training. This has implications when building the model and using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234) # for consistent results\n",
    "model = Sequential(\n",
    "    [               \n",
    "        ### START CODE HERE ### \n",
    "        tf.keras.Input(shape=(400,)),    #specify input shape\n",
    "        Dense(units=25,activation='relu'),\n",
    "        Dense(units=15,activation='relu'),\n",
    "        Dense(units=10,activation='linear'),\n",
    "        ### END CODE HERE ### \n",
    "    ], name = \"my_model\" \n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For the model compilation, the loss and the optimzaer should be defined.\n",
    "    * The SparseCategoricalCrossentropy indicates the softmax should be included with the loss calculation by adding from_logits=True.\n",
    "    * A popular choice for the optimizder is Adaptive Moment (Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the gradient and fit the data.\n",
    "* The first line, Epoch 1/40, describes which epoch the model is currently running. For efficiency, the training data set is broken into 'batches'. The default size of a batch in Tensorflow is 32. There are 5000 examples in our data set or roughly 157 batches. The notation on the 2nd line 157/157 [==== is describing which batch has been executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(\n",
    "    X,y,\n",
    "    epochs=40\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Making a prediction for checking the success of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_of_two = X[1015]\n",
    "\n",
    "prediction = model.predict(image_of_two.reshape(1,400))  # prediction\n",
    "\n",
    "print(f\" predicting a Two: \\n{prediction}\")\n",
    "print(f\" Largest Prediction index: {np.argmax(prediction)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
